{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> NN model with triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import PIL\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from source.network import NNClassifier\n",
    "\n",
    "from source.helpme import get_gesture_dataset, load_imgs_from_folder, show_image, calculate_pad, show_history\n",
    "from source.helpme import create_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = get_gesture_dataset(size=(64, 64), shuffle=True, gray_scale=False)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define transforms that will be applied to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trs = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=1, contrast=1, saturation=0.5, hue=0),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the dataset in three parts: train, valid, test\n",
    "train_size = 1400\n",
    "valid_size = 250\n",
    "test_size = len(X) - train_size - valid_size\n",
    "print('train size =', train_size)\n",
    "print('valid size =', valid_size)\n",
    "print('test size =', test_size)\n",
    "\n",
    "X_train = X[ : train_size]\n",
    "y_train = y[ : train_size]\n",
    "\n",
    "X_valid = X[train_size : train_size+valid_size]\n",
    "y_valid = y[train_size : train_size+valid_size]\n",
    "\n",
    "X_test = X[train_size+valid_size : ]\n",
    "y_test = y[train_size+valid_size : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create three data loaders: train, valid, test\n",
    "train_loader = create_loader(X_train, y_train, batch_size=512, num_workers=1, shuffle=False, trs=trs)\n",
    "valid_loader = create_loader(X_valid, y_valid, batch_size=len(X_valid), num_workers=1, shuffle=False, trs=None)\n",
    "test_loader = create_loader(X_test, y_test, batch_size=len(X_test), num_workers=1, shuffle=False, trs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show random image with applied transforms\n",
    "for x_batch, y_batch in train_loader:\n",
    "    show_image(x_batch[3])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax_layer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e = torch.exp(x - x.max(1, True)[0] )\n",
    "        summ = e.sum(1, True)[0]\n",
    "        return e / summ\n",
    "    \n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        return x.view(N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calculate_pad(input_size = 64, kernel_size = 3, stride = 2, output_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_net = torch.nn.Sequential(torch.nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2, padding=1),\n",
    "                               torch.nn.ReLU(),\n",
    "                               torch.nn.BatchNorm2d(8),\n",
    "                               torch.nn.Dropout2d(0.01),\n",
    "                               # 32x32x8\n",
    "                               \n",
    "                               torch.nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "                               torch.nn.ReLU(),\n",
    "                               torch.nn.BatchNorm2d(16),\n",
    "                               torch.nn.Dropout2d(0.01),\n",
    "                               # 16x16x16\n",
    "                               \n",
    "                               torch.nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "                               torch.nn.ReLU(),\n",
    "                               torch.nn.BatchNorm2d(32),\n",
    "                               torch.nn.Dropout2d(0.01),\n",
    "                               # 8x8x32\n",
    "                               \n",
    "                               torch.nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "                               torch.nn.ReLU(),\n",
    "                               torch.nn.BatchNorm2d(64),\n",
    "                               torch.nn.Dropout2d(0.01),\n",
    "                               # 4x4x64\n",
    "                               \n",
    "                               torch.nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "                               torch.nn.ReLU(),\n",
    "                               torch.nn.BatchNorm2d(128),\n",
    "                               torch.nn.Dropout2d(0.01),\n",
    "                               # 2x2x128\n",
    "                               \n",
    "                               Flatten(),\n",
    "                               torch.nn.Linear(512, 10),\n",
    "                               Softmax_layer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NNClassifier(conv_net, lr=1e-3, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_loader(train_loader, valid_loader, epochs=15, log_every_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_history(model.train_history, model.valid_history, width=2, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate_score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем как перейти к тестированию на реальных данных, надо положить картинки в любом формате в папку source/real_data. Все что там находится можно удалить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs = load_imgs_from_folder(os.path.join('source', 'real_data'), size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_image(imgs[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(conv_net.state_dict(), './my_network.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
